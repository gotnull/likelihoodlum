# ðŸŽ² Likelihoodlum

**Detect whether a GitHub repository's code was likely written by an LLM.**

Likelihoodlum analyzes a repository's commit history and uses timing-based heuristics to estimate the likelihood that the code was generated by a large language model rather than written by a human.

The core idea is simple: **humans type slow, LLMs don't.** If someone is pushing hundreds of lines of polished code every few minutes, something's up.

---

## How It Works

Likelihoodlum fetches commit history via the GitHub API and scores the repo on a 0â€“100 scale across seven heuristic signals. Generated and vendored files (lockfiles, protobufs, Xcode project files, build artifacts, etc.) are **automatically filtered out** so they don't inflate velocity measurements.

### Scoring Signals

| # | Signal | Points | What It Measures |
|---|---|---|---|
| 1 | **Code Velocity** | âˆ’10 to +35 | Lines changed per minute between consecutive commits by the same author. Thresholds: clearly human (~30 LoC/hr), human upper (~90 LoC/hr), suspicious (~240 LoC/hr), very suspicious (~600 LoC/hr). Also rewards clearly human pace with negative points. |
| 2 | **Session Productivity** | âˆ’5 to +20 | Groups commits into coding sessions (>2hr gap = new session) and measures aggregate lines/min. Human-pace sessions actively reduce the score. |
| 3 | **Commit Size Uniformity** | âˆ’5 to +15 | LLM dumps tend to be uniformly large. Human commits vary (small fixes, big features, etc). High variation is rewarded with negative points. |
| 4 | **Commit Message Patterns** | 0 to +15 | Catches generic messages like *"Implement X"*, *"Add Y functionality"*, *"Fix issue with Z"* â€” the kind LLMs love to produce. |
| 5 | **Burst Detection** | 0 to +15 | Flags sessions where >500 authored lines appeared in under 30 minutes. |
| 6 | **Multi-Author Discount** | âˆ’10 to +5 | Real projects tend to have multiple contributors (penalty). Solo-author repos get a small bump. |
| 7 | **Generated File Ratio** | Informational | Reports what percentage of line changes are in generated/vendor files (excluded from analysis). |

> **Note:** The score uses both positive signals (suspicious patterns push the score up) and negative signals (clearly human patterns actively pull it down). The final score is clamped to 0â€“100.

### Velocity Thresholds

| Threshold | Lines/Min | Lines/Hr | Interpretation |
|---|---|---|---|
| Clearly Human | < 0.5 | < 30 | Normal productive human |
| Human Upper | < 1.5 | < 90 | Fast human, maybe some copy-paste |
| Suspicious | â‰¥ 4.0 | â‰¥ 240 | Quite fast, could be assisted |
| Very Suspicious | â‰¥ 10.0 | â‰¥ 600 | Almost certainly not hand-typed |

### Verdicts

| Score | Verdict |
|---|---|
| 75â€“100 | ðŸ¤– Very likely LLM-generated |
| 50â€“74 | ðŸ¤– Likely LLM-assisted |
| 30â€“49 | ðŸ¤” Possibly LLM-assisted |
| 15â€“29 | ðŸ‘¤ Likely human-written |
| 0â€“14 | ðŸ‘¤ Almost certainly human-written |

### Generated File Filtering

The following file types are automatically excluded from velocity and size calculations:

- **Lock files**: `package-lock.json`, `yarn.lock`, `Podfile.lock`, `Cargo.lock`, `go.sum`, etc.
- **Xcode / Apple**: `.pbxproj`, `.xcworkspacedata`, `.xcscheme`
- **Protobuf / codegen**: `.pb.go`, `.pb.swift`, `_pb2.py`, `.g.dart`, `.freezed.dart`, `.generated.*`
- **Build artifacts**: `.min.js`, `.min.css`, `.map`, `dist/`, `build/`, `vendor/`, `node_modules/`
- **Data / assets**: `.json`, `.svg`, `.png`, `.jpg`, `.ico`, fonts

## Installation

**Zero dependencies** â€” runs on Python 3.10+ with only the standard library.

```bash
git clone https://github.com/gotnull/likelihoodlum.git
cd likelihoodlum
```

Optionally install `python-dotenv` for `.env` file support (a built-in fallback parser is included if you don't):

```bash
pip install python-dotenv
```

## Setup

### GitHub Token (Recommended)

Without a token you're limited to 60 API requests/hour. With one, you get 5,000/hr.

**Option A: `.env` file** (recommended)

```bash
cp .env.example .env
# Edit .env and add your token
```

```
GITHUB_TOKEN=ghp_your_token_here
```

The `.env` file is gitignored by default â€” your token stays safe.

**Option B: Environment variable**

```bash
export GITHUB_TOKEN="ghp_your_token_here"
```

**Option C: CLI flag**

```bash
python3 llm_detector.py owner/repo --token ghp_your_token_here
```

### Generating a Token

1. Go to [GitHub â†’ Settings â†’ Developer settings â†’ Personal access tokens](https://github.com/settings/tokens)
2. Generate a new token (classic) with `public_repo` scope (or `repo` for private repos)
3. Copy it â€” you won't see it again

## Usage

```bash
# Basic â€” analyze a public repo
python3 llm_detector.py owner/repo

# Full GitHub URL works too
python3 llm_detector.py https://github.com/owner/repo

# Analyze more commits (default is 200)
python3 llm_detector.py owner/repo --max-commits 1000

# Target a specific branch
python3 llm_detector.py owner/repo --branch develop

# Machine-readable JSON output
python3 llm_detector.py owner/repo --json

# Go big
python3 llm_detector.py owner/repo --max-commits 5000 --json > report.json
```

### CLI Reference

| Flag | Default | Description |
|---|---|---|
| `repo` (positional) | â€” | GitHub repo as `owner/repo` or full URL |
| `--token` | `$GITHUB_TOKEN` | GitHub personal access token |
| `--branch` | repo default | Branch to analyze |
| `--max-commits` | `200` | Maximum number of commits to fetch |
| `--json` | off | Output results as JSON |

### Example Output

```
============================================================
  LLM Code Detector Report
  Repository: someone/suspicious-project
============================================================

ðŸ“Š Commits analyzed: 200
ðŸ“… Time span: 12 days
ðŸ‘¥ Authors: 1
   â€¢ devguy42: 200 commits

ðŸ“ Line changes breakdown:
   Total:     48,230
   Authored:  41,500 (used for analysis)
   Generated: 6,730 (filtered out)

âš¡ Velocity (authored lines/min between commits):
   Median:        9.40  (â‰ˆ 564 lines/hr)
   Trimmed mean:  8.12  (â‰ˆ 487 lines/hr)
   Max:           87.50
   Intervals above suspicious threshold: 143/198

ðŸ”¥ Fastest commit intervals:
   a1b2c3d4â†’e5f6g7h8  1750 lines in 20.0 min = 87.5 l/min âš ï¸
   ...

ðŸ• Coding sessions (gap > 120 min): 15

ðŸ’¬ Commit messages matching LLM patterns: 168/200 (84.0%)
   â€¢ "Add user authentication module"
   â€¢ "Implement API endpoint for data retrieval"
   â€¢ "Update README.md"

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ðŸŽ¯ LLM Likelihood Score: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆÂ·Â·Â·Â·Â·Â·] 82/100
  ðŸ¤– Very likely LLM-generated
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ“ Reasoning:
   â€¢ Median velocity is extremely high (9.4 lines/min â‰ˆ 564 lines/hr)
   â€¢ 72% of intervals show very high velocity
   â€¢ Median session productivity is extreme (11.2 lines/min)
   â€¢ 84% of commit messages match LLM-typical patterns
   â€¢ 6 burst sessions detected (>500 authored lines in <30 min)
   â€¢ Solo author â€” consistent with LLM-assisted workflow [+5]

âš   Disclaimer: This is a heuristic analysis and NOT definitive proof.
   Fast coding can also indicate copy-paste, boilerplate generators,
   IDE scaffolding, or simply an experienced developer.
```

### JSON Output

When using `--json`, the output includes:

```json
{
  "repository": "owner/repo",
  "commits_analyzed": 200,
  "score": 82,
  "verdict": "ðŸ¤– Very likely LLM-generated",
  "reasons": ["..."],
  "velocity_stats": {
    "median_lpm": 9.4,
    "trimmed_mean_lpm": 8.12,
    "intervals": 198
  },
  "line_changes": {
    "total": 48230,
    "authored": 41500,
    "generated": 6730
  },
  "message_analysis": {
    "total": 200,
    "pattern_hits": 168,
    "ratio": 0.84,
    "sample_flagged": ["..."]
  },
  "sessions": 15,
  "authors": 1
}
```

## API Rate Limits

Each commit requires one API call for detailed stats. Keep this in mind:

| Auth | Rate Limit | Max Commits Comfortable |
|---|---|---|
| No token | 60/hr | ~50 |
| With token | 5,000/hr | ~4,000 |

## Limitations & Disclaimer

This tool uses **heuristics, not magic**. A high score doesn't prove LLM usage, and a low score doesn't disprove it.

False positives can come from:
- Copy-pasting code from other projects
- IDE/framework scaffolding and boilerplate generators
- Squashed/rebased commits that compress work
- An experienced developer who plans before coding
- Generated code (protobufs, OpenAPI, etc.)

False negatives can come from:
- LLM-generated code committed slowly or in small chunks
- Human-edited LLM output committed as normal work
- Commits with manual timing that mimics human patterns

**Use responsibly.** This is a curiosity tool, not a courtroom exhibit.

## License

MIT â€” do whatever you want with it.

## Contributing

Found a new heuristic? PRs welcome. Ideas:

- Diff complexity scoring (entropy analysis)
- File-type breakdown (LLMs love generating configs)
- Comment density analysis
- Code style consistency metrics
- Cross-referencing with known LLM output patterns