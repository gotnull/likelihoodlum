# ðŸŽ² Likelihoodlum

**Detect whether a GitHub repository's code was likely written by an LLM.**

Likelihoodlum analyzes a repository's commit history and uses timing-based heuristics to estimate the likelihood that the code was generated by a large language model rather than written by a human.

The core idea is simple: **humans type slow, LLMs don't.** If someone is pushing hundreds of lines of polished code every few minutes â€” or shipping an entire app in a week â€” something's up.

---

## How It Works

Likelihoodlum fetches commit history and repository metadata via the GitHub API, then scores the repo on a 0â€“100 scale across nine heuristic signals. Generated and vendored files (lockfiles, protobufs, Xcode project files, build artifacts, etc.) are **automatically filtered out** so they don't inflate velocity measurements. Bot accounts (e.g. `dependabot[bot]`) are excluded from author counts and velocity calculations.

Commit details are fetched **concurrently** (up to 10 parallel requests) for significantly faster analysis, and bot commits are skipped entirely to save API calls.

### Scoring Signals

| # | Signal | Points | What It Measures |
|---|---|---|---|
| 1 | **Code Velocity** | âˆ’10 to +35 | Lines changed per minute between consecutive commits by the same author. When the trimmed mean is significantly higher than the median (heavy tail of fast intervals), the score is boosted further. |
| 2 | **Session Productivity** | âˆ’5 to +20 | Groups commits into coding sessions (>2hr gap = new session) and measures aggregate lines/min. Human-pace sessions actively reduce the score. |
| 3 | **Commit Size Uniformity** | âˆ’5 to +15 | LLM dumps tend to be uniformly large. Human commits vary in size (small fixes, big features, etc). High variation is rewarded with negative points. |
| 4 | **Commit Message Patterns** | 0 to +15 | Catches generic messages like *"Implement X"*, *"Add Y functionality"*, *"Fix issue with Z"*, conventional commits with verbose scopes or multi-scopes (`feat(a, b):`), and other LLM-typical phrasings. If messages look clean but velocity is very high, a small cross-signal bonus is applied. |
| 5 | **Burst Detection** | 0 to +15 | Flags sessions where >300 authored lines appeared in under 30 minutes (rapid bursts), plus longer sessions with sustained extreme throughput (â‰¥10 lines/min). |
| 6 | **Multi-Author Discount** | âˆ’10 to +5 | Real projects tend to have multiple contributors (score penalty). Solo-author repos get a small bump. Bot accounts are excluded from the count. |
| 7 | **Extreme Per-Commit Velocity** | 0 to +10 | Counts commit intervals exceeding 50 lines/min (~3,000 lines/hr). Even a small percentage of these is a strong signal. |
| 8 | **Project-Scale Plausibility** | âˆ’5 to +20 | The big-picture sanity check. Compares total authored output against the repo's true creation date (fetched from GitHub metadata) and active coding days. A senior engineer produces ~200â€“500 lines of production code per day â€” 10,000+ lines/day sustained over weeks is implausible without LLM assistance. |
| 9 | **Generated File Ratio** | Informational | Reports what percentage of line changes are in generated/vendor files (excluded from all calculations above). |

> **Note:** The score uses both positive signals (suspicious patterns push the score up) and negative signals (clearly human patterns actively pull it down). The final score is clamped to 0â€“100.

### Velocity Thresholds

| Threshold | Lines/Min | Lines/Hr | Interpretation |
|---|---|---|---|
| Clearly Human | < 0.5 | < 30 | Normal productive human |
| Human Upper | < 1.5 | < 90 | Fast human, maybe some copy-paste |
| Suspicious | â‰¥ 4.0 | â‰¥ 240 | Quite fast, could be assisted |
| Very Suspicious | â‰¥ 10.0 | â‰¥ 600 | Almost certainly not hand-typed |

### Daily Output Thresholds

| Lines/Active Day | Interpretation | Points |
|---|---|---|
| < 300 | Normal human pace | âˆ’5 (if span â‰¥ 14 days) |
| 300â€“799 | Fast but plausible | 0 |
| 800â€“1,999 | Above average | +5 |
| 2,000â€“4,999 | Very high, likely assisted | +12 |
| â‰¥ 5,000 | Implausible for a human | +20 |

### Verdicts

| Score | Verdict |
|---|---|
| 75â€“100 | ðŸ¤– Very likely LLM-generated |
| 50â€“74 | ðŸ¤– Likely LLM-assisted |
| 30â€“49 | ðŸ¤” Possibly LLM-assisted |
| 15â€“29 | ðŸ‘¤ Likely human-written |
| 0â€“14 | ðŸ‘¤ Almost certainly human-written |

### Generated File Filtering

The following file types are automatically excluded from velocity, size, and daily output calculations:

- **Lock files**: `package-lock.json`, `yarn.lock`, `Podfile.lock`, `Cargo.lock`, `go.sum`, etc.
- **Xcode / Apple**: `.pbxproj`, `.xcworkspacedata`, `.xcscheme`
- **Protobuf / codegen**: `.pb.go`, `.pb.swift`, `_pb2.py`, `.g.dart`, `.freezed.dart`, `.generated.*`
- **Build artifacts**: `.min.js`, `.min.css`, `.map`, `dist/`, `build/`, `vendor/`, `node_modules/`
- **Data / assets**: `.json`, `.svg`, `.png`, `.jpg`, `.ico`, fonts

### Bot Author Filtering

Authors matching known bot patterns (e.g. `dependabot[bot]`, `renovate-bot`) are automatically:

- Excluded from velocity and session calculations
- Excluded from author counts (so a solo dev + dependabot is correctly identified as a solo author)
- Skipped during commit detail fetching (saves API calls)

## Installation

**Zero dependencies** â€” runs on Python 3.10+ with only the standard library.

```bash
git clone https://github.com/gotnull/likelihoodlum.git
cd likelihoodlum
```

Optionally install `python-dotenv` for `.env` file support (a built-in fallback parser is included if you don't):

```bash
pip install python-dotenv
```

## Setup

### GitHub Token (Recommended)

Without a token you're limited to 60 API requests/hour. With one, you get 5,000/hr.

**Option A: `.env` file** (recommended)

```bash
cp .env.example .env
# Edit .env and add your token
```

```
GITHUB_TOKEN=ghp_your_token_here
```

The `.env` file is gitignored by default â€” your token stays safe.

**Option B: Environment variable**

```bash
export GITHUB_TOKEN="ghp_your_token_here"
```

**Option C: CLI flag**

```bash
python3 llm_detector.py owner/repo --token ghp_your_token_here
```

### Generating a Token

1. Go to [GitHub â†’ Settings â†’ Developer settings â†’ Personal access tokens](https://github.com/settings/tokens)
2. Generate a new token (classic) with `public_repo` scope (or `repo` for private repos)
3. Copy it â€” you won't see it again

## Usage

```bash
# Basic â€” analyze a public repo
python3 llm_detector.py owner/repo

# Full GitHub URL works too
python3 llm_detector.py https://github.com/owner/repo

# Analyze more commits (default is 200)
python3 llm_detector.py owner/repo --max-commits 1000

# Target a specific branch
python3 llm_detector.py owner/repo --branch develop

# Machine-readable JSON output
python3 llm_detector.py owner/repo --json

# Go big
python3 llm_detector.py owner/repo --max-commits 5000 --json > report.json
```

### CLI Reference

| Flag | Default | Description |
|---|---|---|
| `repo` (positional) | â€” | GitHub repo as `owner/repo` or full URL |
| `--token` | `$GITHUB_TOKEN` | GitHub personal access token |
| `--branch` | repo default | Branch to analyze |
| `--max-commits` | `200` | Maximum number of commits to fetch |
| `--json` | off | Output results as JSON |

### Example Output

```
============================================================
  LLM Code Detector Report
  Repository: someone/suspicious-project
============================================================

ðŸ“Š Commits analyzed: 195
ðŸ“… Time span: 30 days
ðŸ‘¥ Authors: 1
   â€¢ devguy42: 195 commits

ðŸ“ Line changes breakdown:
   Total:     144,420
   Authored:  141,780 (used for analysis)
   Generated: 644 (filtered out)

ðŸ“ˆ Project-scale output:
   Repo created:         2025-01-15
   Active days:          14 (of 90 calendar days)
   Lines/active day:     10,127
   Lines/calendar day:   1,571
    âš ï¸  (>5,000 â€” implausible for a human)

âš¡ Velocity (authored lines/min between commits):
   Median:        5.51  (â‰ˆ 331 lines/hr)
   Trimmed mean:  9.64  (â‰ˆ 578 lines/hr)
   Max:           539.05
   Intervals above suspicious threshold: 101/181

ðŸ”¥ Fastest commit intervals:
   a1b2c3d4â†’e5f6g7h8  4555 lines in 8.4 min = 539.05 l/min âš ï¸
   b2c3d4e5â†’f6g7h8i9  8890 lines in 26.5 min = 335.47 l/min âš ï¸
   ...

ðŸ• Coding sessions (gap > 120 min): 27

ðŸ’¬ Commit messages matching LLM patterns: 138/195 (70.8%)
   â€¢ "feat(admin): add auth token refresh and improve icon picker UI"
   â€¢ "feat(settings): add error handling for device disconnection"
   â€¢ "fix(api): update endpoint to handle pagination correctly"

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ðŸŽ¯ LLM Likelihood Score: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100/100
  ðŸ¤– Very likely LLM-generated
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ“ Reasoning:
   â€¢ Median velocity is suspiciously high (5.5 lines/min â‰ˆ 331 lines/hr)
   â€¢ Trimmed mean (9.6 l/min) is 1.7Ã— the median â€” heavy tail of fast intervals
   â€¢ 35% of intervals show high velocity
   â€¢ Median session productivity is extreme (26.8 lines/min)
   â€¢ Commit sizes vary widely â€” typical of human work (CV=2.30) [-5]
   â€¢ 71% of commit messages match LLM-typical patterns
   â€¢ 20 burst/high-velocity sessions detected
   â€¢ Solo author â€” consistent with LLM-assisted workflow [+5]
   â€¢ 14 commit intervals (8%) show extreme velocity (>50 lines/min) [+2]
   â€¢ Project-scale output is implausible: 141,780 authored lines over 90 days
     (14 active) = 10,127 lines/active day [+20]

âš   Disclaimer: This is a heuristic analysis and NOT definitive proof.
   Fast coding can also indicate copy-paste, boilerplate generators,
   IDE scaffolding, or simply an experienced developer.
```

### JSON Output

When using `--json`, the output includes:

```json
{
  "repository": "owner/repo",
  "commits_analyzed": 195,
  "score": 100,
  "verdict": "ðŸ¤– Very likely LLM-generated",
  "reasons": ["..."],
  "velocity_stats": {
    "median_lpm": 5.51,
    "trimmed_mean_lpm": 9.64,
    "intervals": 181
  },
  "line_changes": {
    "total": 144420,
    "authored": 141780,
    "generated": 644
  },
  "project_scale": {
    "repo_created_at": "2025-01-15T00:00:00+00:00",
    "calendar_days": 90,
    "active_days": 14,
    "lines_per_active_day": 10127
  },
  "message_analysis": {
    "total": 195,
    "pattern_hits": 138,
    "ratio": 0.708,
    "sample_flagged": ["..."]
  },
  "sessions": 27,
  "authors": 1
}
```

## API Usage

The tool fetches repo metadata (1 call), commit listings (1â€“N calls depending on page count), and detailed stats per non-bot commit. Bot commits are skipped to save calls. Detail requests are made concurrently (up to 10 in parallel) for faster analysis.

| Auth | Rate Limit | Max Commits Comfortable |
|---|---|---|
| No token | 60/hr | ~50 |
| With token | 5,000/hr | ~4,000 |

## Limitations & Disclaimer

This tool uses **heuristics, not magic**. A high score doesn't prove LLM usage, and a low score doesn't disprove it.

False positives can come from:
- Copy-pasting code from other projects
- IDE/framework scaffolding and boilerplate generators
- Squashed/rebased commits that compress work
- Merge commits (maintainers merging large PRs)
- An experienced developer who plans before coding
- Generated code (protobufs, OpenAPI, etc.)

False negatives can come from:
- LLM-generated code committed slowly or in small chunks
- Human-edited LLM output committed as normal work
- Commits with manual timing that mimics human patterns
- Repos where `--max-commits` doesn't capture the full picture

**Use responsibly.** This is a curiosity tool, not a courtroom exhibit.

## License

MIT â€” do whatever you want with it.

## Contributing

Found a new heuristic? PRs welcome. Ideas:

- Diff complexity scoring (entropy analysis)
- File-type breakdown (LLMs love generating configs)
- Comment density analysis
- Code style consistency metrics
- Cross-referencing with known LLM output patterns
- Language-specific signal tuning