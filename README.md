# ðŸŽ² Likelihoodlum

[![GitHub stars](https://img.shields.io/github/stars/gotnull/likelihoodlum?style=social)](https://github.com/gotnull/likelihoodlum/stargazers)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Zero Dependencies](https://img.shields.io/badge/dependencies-zero-brightgreen.svg)](#installation)

**Detect whether a GitHub repository's code was likely written by an LLM.**

Likelihoodlum analyzes a repository's commit history and uses timing-based heuristics to estimate the likelihood that the code was generated by a large language model rather than written by a human.

The core idea is simple: **humans type slow, LLMs don't.** If someone is pushing hundreds of lines of polished code every few minutes â€” or shipping an entire app in a week â€” something's up.

---

## ðŸ† The Wall of Truth

Real results from real repos. Every score below was generated by Likelihoodlum with default settings (`--max-commits 200`).

### ðŸ¤– Caught Red-Handed

| Repository | â­ Stars | Score | Verdict | Daily Output | Velocity | Authors |
|---|---|---|---|---|---|---|
| [anthropics/claudes-c-compiler](https://github.com/anthropics/claudes-c-compiler) | 2,399 | **81/100** | ðŸ¤– Very likely LLM-generated | 60,638 lines in 1 day | 7.6 l/min median | `claude` (198 commits) |

> *Anthropic literally named the author `claude`. 198 of 200 commits. 60K lines in a single day. The tool didn't even break a sweat on this one.*

### ðŸ¤” Suspicious â€” You Decide

| Repository | â­ Stars | Score | Verdict | Key Signals |
|---|---|---|---|---|
| [openai/codex](https://github.com/openai/codex) | â€” | **35/100** | ðŸ¤” Possibly LLM-assisted | 9,562 lines/active day, 17% LLM message patterns |
| [jlowin/fastmcp](https://github.com/jlowin/fastmcp) | â€” | **34/100** | ðŸ¤” Possibly LLM-assisted | 3,046 lines/active day, extreme session productivity |
| [rust-lang/rust](https://github.com/rust-lang/rust) | â€” | **32/100** | ðŸ¤” Possibly LLM-assisted | 7,032 lines/active day (merge commits inflate this) |
| [Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) | â€” | **29/100** | ðŸ‘¤ Likely human-written | 74% LLM message patterns, but 14 authors + low velocity |
| [microsoft/vscode](https://github.com/microsoft/vscode) | â€” | **28/100** | ðŸ‘¤ Likely human-written | Large merge commits inflate daily output |
| [twitter/the-algorithm](https://github.com/twitter/the-algorithm) | â€” | **26/100** | ðŸ‘¤ Likely human-written | 13,379 lines/active day â€” bulk repo dump |

### ðŸ‘¤ Certified Human

| Repository | â­ Stars | Score | Verdict | Median Velocity | Authors | LLM Messages |
|---|---|---|---|---|---|---|
| [vuejs/core](https://github.com/vuejs/core) | â€” | **20/100** | ðŸ‘¤ Likely human-written | 0.0 l/min | 52 | 66%* |
| [pallets/flask](https://github.com/pallets/flask) | â€” | **17/100** | ðŸ‘¤ Likely human-written | â€” | 23 | 0% |
| [meshtastic/meshtastic-apple](https://github.com/meshtastic/meshtastic-apple) | â€” | **16/100** | ðŸ‘¤ Likely human-written | 0.3 l/min | 8 | 4% |
| [denoland/deno](https://github.com/denoland/deno) | â€” | **15/100** | ðŸ‘¤ Likely human-written | 0.1 l/min | 41 | 56%* |
| [langchain-ai/langchain](https://github.com/langchain-ai/langchain) | â€” | **15/100** | ðŸ‘¤ Likely human-written | 0.1 l/min | 44 | 57%* |
| [vercel/next.js](https://github.com/vercel/next.js) | â€” | **14/100** | ðŸ‘¤ Almost certainly human-written | 0.2 l/min | 29 | â€” |
| [facebook/react](https://github.com/facebook/react) | â€” | **10/100** | ðŸ‘¤ Almost certainly human-written | 0.1 l/min | 29 | â€” |
| [pydantic/pydantic](https://github.com/pydantic/pydantic) | â€” | **10/100** | ðŸ‘¤ Almost certainly human-written | 0.1 l/min | 47 | 18% |
| [stackblitz/bolt.new](https://github.com/stackblitz/bolt.new) | â€” | **2/100** | ðŸ‘¤ Almost certainly human-written | 0.1 l/min | 17 | 4% |
| [godotengine/godot](https://github.com/godotengine/godot) | â€” | **2/100** | ðŸ‘¤ Almost certainly human-written | 0.1 l/min | 51 | â€” |
| [golang/go](https://github.com/golang/go) | â€” | **0/100** | ðŸ‘¤ Almost certainly human-written | 0.0 l/min | 85 | 0% |
| [django/django](https://github.com/django/django) | â€” | **0/100** | ðŸ‘¤ Almost certainly human-written | 0.0 l/min | 73 | 0% |
| [bitcoin/bitcoin](https://github.com/bitcoin/bitcoin) | â€” | **0/100** | ðŸ‘¤ Almost certainly human-written | 0.1 l/min | 23 | 0% |
| [sveltejs/svelte](https://github.com/sveltejs/svelte) | â€” | **0/100** | ðŸ‘¤ Almost certainly human-written | 0.1 l/min | 36 | 1% |
| [tinygrad/tinygrad](https://github.com/tinygrad/tinygrad) | â€” | **0/100** | ðŸ‘¤ Almost certainly human-written | 0.1 l/min | 15 | â€” |
| [nixos/nixpkgs](https://github.com/nixos/nixpkgs) | â€” | **0/100** | ðŸ‘¤ Almost certainly human-written | â€” | 41 | 0% |
| [torvalds/linux](https://github.com/torvalds/linux) | â€” | **41/100**â€  | ðŸ¤” Possibly LLM-assisted | 1.6 l/min | 35 | 0% |

> \* High message pattern % from conventional commit style (`feat():`, `fix():`), not actual LLM usage.
>
> â€  Linux kernel scores higher than expected because Torvalds merges massive subsystem PRs â€” each merge looks like thousands of lines appearing instantly. The multi-author discount (âˆ’10) keeps it in check.

### ðŸ”¬ The Anthropic Spotlight

Because who better to test an LLM detector on than the company *making* the LLMs?

| Repository | Score | Verdict | Notes |
|---|---|---|---|
| [anthropics/claude-code](https://github.com/anthropics/claude-code) | **0/100** | ðŸ‘¤ Almost certainly human-written | 20 authors, human velocity, CV=7.62. Real team, real software. |
| [anthropics/anthropic-sdk-python](https://github.com/anthropics/anthropic-sdk-python) | **0/100** | ðŸ‘¤ Almost certainly human-written | 159/200 commits were bots (filtered out). The 41 human commits? Glacial pace. |
| [anthropics/claude-code-action](https://github.com/anthropics/claude-code-action) | **0/100** | ðŸ‘¤ Almost certainly human-written | 35 authors, every negative signal fired. |
| [anthropics/skills](https://github.com/anthropics/skills) | **3/100** | ðŸ‘¤ Almost certainly human-written | 10K lines/active day triggered +20, but âˆ’25 in negatives crushed it. |
| [modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers) | **0/100** | ðŸ‘¤ Almost certainly human-written | 39 authors. Community-driven. |
| [anthropics/claudes-c-compiler](https://github.com/anthropics/claudes-c-compiler) | **81/100** | ðŸ¤– Very likely LLM-generated | The one that proves the tool works. Author is literally `claude`. |

> *The irony: the company building the most capable coding LLM in the world writes their own code by hand. Except when they let Claude write a C compiler for fun â€” and the tool caught it instantly.*

---

## How It Works

Likelihoodlum fetches commit history and repository metadata via the GitHub API, then scores the repo on a 0â€“100 scale across nine heuristic signals. Generated and vendored files (lockfiles, protobufs, Xcode project files, build artifacts, etc.) are **automatically filtered out** so they don't inflate velocity measurements. Bot accounts (e.g. `dependabot[bot]`) are excluded from author counts and velocity calculations.

Commit details are fetched **concurrently** (up to 10 parallel requests) for significantly faster analysis, and bot commits are skipped entirely to save API calls.

### Scoring Signals

| # | Signal | Points | What It Measures |
|---|---|---|---|
| 1 | **Code Velocity** | âˆ’10 to +35 | Lines changed per minute between consecutive commits by the same author. When the trimmed mean is significantly higher than the median (heavy tail of fast intervals), the score is boosted further. |
| 2 | **Session Productivity** | âˆ’5 to +20 | Groups commits into coding sessions (>2hr gap = new session) and measures aggregate lines/min. Human-pace sessions actively reduce the score. |
| 3 | **Commit Size Uniformity** | âˆ’5 to +15 | LLM dumps tend to be uniformly large. Human commits vary in size (small fixes, big features, etc). High variation is rewarded with negative points. |
| 4 | **Commit Message Patterns** | 0 to +15 | Catches generic messages like *"Implement X"*, *"Add Y functionality"*, *"Fix issue with Z"*, conventional commits with verbose scopes or multi-scopes (`feat(a, b):`), and other LLM-typical phrasings. If messages look clean but velocity is very high, a small cross-signal bonus is applied. |
| 5 | **Burst Detection** | 0 to +15 | Flags sessions where >300 authored lines appeared in under 30 minutes (rapid bursts), plus longer sessions with sustained extreme throughput (â‰¥10 lines/min). |
| 6 | **Multi-Author Discount** | âˆ’10 to +5 | Real projects tend to have multiple contributors (score penalty). Solo-author repos get a small bump. Bot accounts are excluded from the count. |
| 7 | **Extreme Per-Commit Velocity** | 0 to +10 | Counts commit intervals exceeding 50 lines/min (~3,000 lines/hr). Even a small percentage of these is a strong signal. |
| 8 | **Project-Scale Plausibility** | âˆ’5 to +20 | The big-picture sanity check. Compares total authored output against the repo's true creation date (fetched from GitHub metadata) and active coding days. A senior engineer produces ~200â€“500 lines of production code per day â€” 10,000+ lines/day sustained over weeks is implausible without LLM assistance. |
| 9 | **Generated File Ratio** | Informational | Reports what percentage of line changes are in generated/vendor files (excluded from all calculations above). |

> **Note:** The score uses both positive signals (suspicious patterns push the score up) and negative signals (clearly human patterns actively pull it down). The final score is clamped to 0â€“100.

### Velocity Thresholds

| Threshold | Lines/Min | Lines/Hr | Interpretation |
|---|---|---|---|
| Clearly Human | < 0.5 | < 30 | Normal productive human |
| Human Upper | < 1.5 | < 90 | Fast human, maybe some copy-paste |
| Suspicious | â‰¥ 4.0 | â‰¥ 240 | Quite fast, could be assisted |
| Very Suspicious | â‰¥ 10.0 | â‰¥ 600 | Almost certainly not hand-typed |

### Daily Output Thresholds

| Lines/Active Day | Interpretation | Points |
|---|---|---|
| < 300 | Normal human pace | âˆ’5 (if span â‰¥ 14 days) |
| 300â€“799 | Fast but plausible | 0 |
| 800â€“1,999 | Above average | +5 |
| 2,000â€“4,999 | Very high, likely assisted | +12 |
| â‰¥ 5,000 | Implausible for a human | +20 |

### Verdicts

| Score | Verdict |
|---|---|
| 75â€“100 | ðŸ¤– Very likely LLM-generated |
| 50â€“74 | ðŸ¤– Likely LLM-assisted |
| 30â€“49 | ðŸ¤” Possibly LLM-assisted |
| 15â€“29 | ðŸ‘¤ Likely human-written |
| 0â€“14 | ðŸ‘¤ Almost certainly human-written |

### Generated File Filtering

The following file types are automatically excluded from velocity, size, and daily output calculations:

- **Lock files**: `package-lock.json`, `yarn.lock`, `Podfile.lock`, `Cargo.lock`, `go.sum`, etc.
- **Xcode / Apple**: `.pbxproj`, `.xcworkspacedata`, `.xcscheme`
- **Protobuf / codegen**: `.pb.go`, `.pb.swift`, `_pb2.py`, `.g.dart`, `.freezed.dart`, `.generated.*`
- **Build artifacts**: `.min.js`, `.min.css`, `.map`, `dist/`, `build/`, `vendor/`, `node_modules/`
- **Data / assets**: `.json`, `.svg`, `.png`, `.jpg`, `.ico`, fonts

### Bot Author Filtering

Authors matching known bot patterns (e.g. `dependabot[bot]`, `renovate-bot`) are automatically:

- Excluded from velocity and session calculations
- Excluded from author counts (so a solo dev + dependabot is correctly identified as a solo author)
- Skipped during commit detail fetching (saves API calls)

## Installation

**Zero dependencies** â€” runs on Python 3.10+ with only the standard library.

```bash
git clone https://github.com/gotnull/likelihoodlum.git
cd likelihoodlum
```

Optionally install `python-dotenv` for `.env` file support (a built-in fallback parser is included if you don't):

```bash
pip install python-dotenv
```

## Setup

### GitHub Token (Recommended)

Without a token you're limited to 60 API requests/hour. With one, you get 5,000/hr.

**Option A: `.env` file** (recommended)

```bash
cp .env.example .env
# Edit .env and add your token
```

```
GITHUB_TOKEN=ghp_your_token_here
```

The `.env` file is gitignored by default â€” your token stays safe.

**Option B: Environment variable**

```bash
export GITHUB_TOKEN="ghp_your_token_here"
```

**Option C: CLI flag**

```bash
python3 llm_detector.py owner/repo --token ghp_your_token_here
```

### Generating a Token

1. Go to [GitHub â†’ Settings â†’ Developer settings â†’ Personal access tokens](https://github.com/settings/tokens)
2. Generate a new token (classic) with `public_repo` scope (or `repo` for private repos)
3. Copy it â€” you won't see it again

## Usage

```bash
# Basic â€” analyze a public repo
python3 llm_detector.py owner/repo

# Full GitHub URL works too
python3 llm_detector.py https://github.com/owner/repo

# Analyze more commits (default is 200)
python3 llm_detector.py owner/repo --max-commits 1000

# Target a specific branch
python3 llm_detector.py owner/repo --branch develop

# Machine-readable JSON output
python3 llm_detector.py owner/repo --json

# Go big
python3 llm_detector.py owner/repo --max-commits 5000 --json > report.json
```

### CLI Reference

| Flag | Default | Description |
|---|---|---|
| `repo` (positional) | â€” | GitHub repo as `owner/repo` or full URL |
| `--token` | `$GITHUB_TOKEN` | GitHub personal access token |
| `--branch` | repo default | Branch to analyze |
| `--max-commits` | `200` | Maximum number of commits to fetch |
| `--json` | off | Output results as JSON |

### Example Output

```
============================================================
  LLM Code Detector Report
  Repository: anthropics/claudes-c-compiler
============================================================

ðŸ“Š Commits analyzed: 200
ðŸ“… Time span: 0 days
ðŸ‘¥ Authors: 2
   â€¢ claude: 198 commits
   â€¢ carlini: 2 commits

ðŸ“ Line changes breakdown:
   Total:     60,638
   Authored:  60,638 (used for analysis)
   Generated: 0 (filtered out)

ðŸ“ˆ Project-scale output:
   Repo created:         2026-02-04
   Active days:          1 (of 1 calendar days)
   Lines/active day:     60,638
   Lines/calendar day:   60,638
    âš ï¸  (>5,000 â€” implausible for a human)

âš¡ Velocity (authored lines/min between commits):
   Median:        7.61  (â‰ˆ 457 lines/hr)
   Trimmed mean:  12.14  (â‰ˆ 728 lines/hr)
   Max:           630.41
   Intervals above suspicious threshold: 69/101

ðŸ”¥ Fastest commit intervals:
   12d83516â†’dc196034  767 lines in 1.2 min = 630.41 l/min âš ï¸
   f2ac8159â†’8fe4994a  163 lines in 1.0 min = 160.33 l/min âš ï¸
   734d5fabâ†’e836df40  1029 lines in 7.2 min = 143.25 l/min âš ï¸
   ...

ðŸ• Coding sessions (gap > 120 min): 2

ðŸ’¬ Commit messages matching LLM patterns: 4/200 (2.0%)
   â€¢ "Update x86 assembler README: fix stale line counts"
   â€¢ "Add missing ARM64 system registers to assembler"

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ðŸŽ¯ LLM Likelihood Score: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆÂ·Â·Â·Â·Â·Â·] 81/100
  ðŸ¤– Very likely LLM-generated
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ“ Reasoning:
   â€¢ Median velocity is suspiciously high (7.6 lines/min â‰ˆ 457 lines/hr)
   â€¢ Trimmed mean (12.1 l/min) is 1.6Ã— the median â€” heavy tail of fast intervals
   â€¢ 44% of intervals show very high velocity
   â€¢ Median session productivity is extreme (78.5 lines/min)
   â€¢ Commit sizes vary widely â€” typical of human work (CV=8.97) [-5]
   â€¢ Commit messages look clean but velocity is high â€” possible curated LLM workflow
   â€¢ 12 commit intervals (12%) show extreme velocity (>50 lines/min) [+4]
   â€¢ Project-scale output is implausible: 60,638 authored lines over 1 days
     (1 active) = 60,638 lines/active day [+20]

âš   Disclaimer: This is a heuristic analysis and NOT definitive proof.
   Fast coding can also indicate copy-paste, boilerplate generators,
   IDE scaffolding, or simply an experienced developer.
```

### JSON Output

When using `--json`, the output includes:

```json
{
  "repository": "owner/repo",
  "commits_analyzed": 200,
  "score": 81,
  "verdict": "ðŸ¤– Very likely LLM-generated",
  "reasons": ["..."],
  "velocity_stats": {
    "median_lpm": 7.61,
    "trimmed_mean_lpm": 12.14,
    "intervals": 101
  },
  "line_changes": {
    "total": 60638,
    "authored": 60638,
    "generated": 0
  },
  "project_scale": {
    "repo_created_at": "2026-02-04T00:00:00+00:00",
    "calendar_days": 1,
    "active_days": 1,
    "lines_per_active_day": 60638
  },
  "message_analysis": {
    "total": 200,
    "pattern_hits": 4,
    "ratio": 0.02,
    "sample_flagged": ["..."]
  },
  "sessions": 2,
  "authors": 2
}
```

## API Usage

The tool fetches repo metadata (1 call), commit listings (1â€“N calls depending on page count), and detailed stats per non-bot commit. Bot commits are skipped to save calls. Detail requests are made concurrently (up to 10 in parallel) for faster analysis.

| Auth | Rate Limit | Max Commits Comfortable |
|---|---|---|
| No token | 60/hr | ~50 |
| With token | 5,000/hr | ~4,000 |

## Limitations & Disclaimer

This tool uses **heuristics, not magic**. A high score doesn't prove LLM usage, and a low score doesn't disprove it.

False positives can come from:
- Copy-pasting code from other projects
- IDE/framework scaffolding and boilerplate generators
- Squashed/rebased commits that compress work
- Merge commits (maintainers merging large PRs)
- An experienced developer who plans before coding
- Generated code (protobufs, OpenAPI, etc.)

False negatives can come from:
- LLM-generated code committed slowly or in small chunks
- Human-edited LLM output committed as normal work
- Commits with manual timing that mimics human patterns
- Repos where `--max-commits` doesn't capture the full picture

**Use responsibly.** This is a curiosity tool, not a courtroom exhibit.

## License

MIT â€” do whatever you want with it.

## Contributing

Found a new heuristic? PRs welcome. Ideas:

- Diff complexity scoring (entropy analysis)
- File-type breakdown (LLMs love generating configs)
- Comment density analysis
- Code style consistency metrics
- Cross-referencing with known LLM output patterns
- Language-specific signal tuning

---

*Built with vibes and a healthy suspicion of anyone committing 10,000 lines a day.* ðŸŽ²